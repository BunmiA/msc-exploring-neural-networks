
    \chapter{Results}
  \section{LRN vs Batch normalization}
The table \ref{lrnvsbatch} shows the training performance of the models using both batch normalization and local response normalization. From the table below it is clear to see batch normalization is superior normalization. This is as expected because as  dicussed in \citep{ioffe2015batch}  batch normalization tends to achieves the same accuracy with 14 times fewer training steps. There is also the possibility that the local response normalization did not work well because of issues with the implementation or the wrong parameters use for training such as an inefficient learning rate. There  epoch levels for the more complex models like the slow fusion and late fusion were less because they took about 3 plus hours to train per epooch hence the were not able to complet under the clock time. 
\begin{table}[h!]
\centering
\begin{tabular}{ |l|c|c|c| } 
 \hline
 Model & Epoch & Accuracy & Loss \\ 
  \hline
 Single frame LRN & 10 & 0.0479 & 4.2867 \\ 
 Single frame batch  & 10 & 0.9158 & 0.3518 \\ 
 Early fusion LRN & 10 & 0.0893 & 3.9725 \\
 Sarly fusion batch & 10 & 0.9467 &  0.2447  \\
 Late fusion LRN & 3 & 0.0194 &  4.6014  \\
 Late fusion batch & 3 & 0.3929  & 2.5302 \\
 Slow fusion LRN & 3 & 0.0246 & 4.5663  \\
 Slow fusion batch & 3 & 0.3764  & 2.5714  \\
 \hline
\end{tabular}
\caption{Batch vs LRN normalization}
\label{lrnvsbatch}
\end{table}

\section{Temporal relationships}
Table \ref{fusions} compares the performance of the single frame, early fusion, late fusion and slow fusion models against the test dataset. Originally all models were planned to run for the same for about 20 epochs using the same early stopping at a 99\% accuracy, but because of the long-running time of the slow and late fusion models all models they were ran for 10 epochs. The performance of the slow fusion model on the training data at 10 epoch was a loss of 0.6477 and then an accuracy of 0.8765.  For the late fusion model, it had a loss of 0.2779  and an accuracy 0.9477.  The early fusion and single frame fusion model  both stopped at 16 epochs because of the early stopping setup. The early fusion had a loss of 0.0179  and an accuracy 0.9995 after 16 epochs while the single frame model had a loss of 0.0186 and an accuracy 0.9994.
From table Table \ref{fusions} we can see that all the models clearly overfitted with the early fusion model performing best then the sing frame model.  Which is not at all similar to the models in \citep{KarpathyCVPR14}, were the flow fusion on average performed better followed by the single-frame fusion model then the late fusion model the finally, the early fusion model. The models as described in \citep{KarpathyCVPR14}, generally did better than the models in the experiment; this could be because of wrong implementation and inefficient hyperparameters and optimization methods.  The overfitting compared to the \citep{KarpathyCVPR14} could also be because of the large amount of augmented varied data used for training in \citep{KarpathyCVPR14}  vs the small non augmented dataset used in this project.
 
\begin{table}[h!]
\centering
\begin{tabular}{ |l|c|c|c| } 
 \hline
 Model & Epoch & Accuracy & Loss \\ 
  \hline
 Single frame batch  & 16   & 0.2360 & 6.3380 \\ 
 Early fusion batch  & 16 & 0.2437 &  6.4750 \\
 Late fusion batch   & 10 & 0.2115 & 5.5500 \\
Slow fusion batch   & 10 & 0.1961& 5.8697\\
 \hline
\end{tabular}
\caption{ Performance of single frame, early fusion, late fusion and slow fusions models against the test dataset}
\label{fusions}
\end{table}

\section{Optimization}
%todo mention next step is parameter searching using different techniques
There was not much improvement in the loss using other optimization method as seen in table \ref{optim}.  This would be most likely due to inefficient hyperparameters used for both the Adam and the RMS prop optimization methods. The RMS prop optimization was cut short due to some issues during running but bad performace can be seeen even at 7 epochs.
\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
  \multicolumn{2}{|c|}{ } &
 \multicolumn{2}{|c|}{Training } &
 \multicolumn{2}{|c|}{validation} \\
 \hline
 model & epoch & accuracy & loss & accuracy & loss \\ 
  \hline
 SGD & 10 &  0.9546 & 0.2101  & 0.2233 &5.4826 \\ 
 Adam & 10 &0.0111 & 15.9395 & 0.01084 &15.9446 \\
 RMS-prop & 7 & 0.0073 & 15.9985 & - & -  \\ 
 \hline
\end{tabular}
\caption{Results of single frame model performance using sgd with momentum, adam and RMS-prop optimization methods}
\label{optim}
\end{table}


\section{Random Single Frame}
The table \ref{randomSingleFrame} shows the performance of the single frame model which randomly selects a frame from the first 25 frames for each training example. This model did exceeding well that all other models based off the models in \citep{KarpathyCVPR14}.  The training data has an accuracy of 70\%, and the test data had an accuracy of about 54\%. There is still a bit of overfitting however, not compared to the single frame model. The single frame model was also trained using the 7th frame from the videos as this was another assumption of where the frames were taken from based on the images from \citep{KarpathyCVPR14}. The 7th frame single frame model also did not do nearly as well of the test set. There is still alot to be investigated here as it proposes that rather than looking at temporal features it might be better to have a diverse set of images for each action for training.  The bad performances of the single-frame model at the first frame and the 7th frame could also be due to a need for an earlier stopping as the random single frame model trained to about 70\% accuracy. 

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
  \multicolumn{2}{|c|}{ } &
 \multicolumn{2}{|c|}{Training } &
 \multicolumn{2}{|c|}{validation} \\
 \hline
 model & epoch & accuracy & loss & accuracy & loss \\ 
  \hline
   Random  single frame model& 20 &  0.7033 & 1.0837 & 0.5388 &1.7621\\ 
Single frame model @ 1  & 10 &   0.9546 & 0.2101 & 0.2234 & 5.4826\\ 
Single frame model @ 7  & 10 &   0.9455 & 0.2345 & 0.2086 &5.3418\\ 
 \hline
\end{tabular}
\caption{Performance of the randon single frame model against a single frame model using the first frame (@1) and the 7th frame(@7)}
\label{randomSingleFrame}
\end{table}


\section{ Performance of pre-trained models}
Table \ref{pretrained} below shows the results from using the pre-trained models; Inception, VGG19, MobileNetv2 and ResNet. The models all ran for 3 epoch, and all had about the same accuracy, which was about 1\% for both the training and test data.  This could because more regularization was needed for these more complex models. For better performance, the models would also have needed to run for a longer time, probably with a higher learning rate or a better optimization method to improve the loss after each epoch.  It was also discovered that the models where accidentally ran with an RMS prop optimization and a learning rate of  0.1 with $\rho$ set to 0.9, and the decay set to 0.0. These parameters must have been extremely inefficient, leading to the bad performance. 

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
  \multicolumn{2}{|c|}{ } &
 \multicolumn{2}{|c|}{Training } &
 \multicolumn{2}{|c|}{validation} \\
 \hline
 model & epoch & accuracy & loss & accuracy & loss \\ 
  \hline
     Inception & 3 & 0.0106 & 748.3878 & 0.0116&749.3610 \\ 
 MobileNetv2 & 3 &0.0106 &  745.1055  & 0.0116&749.3610\\
  ResNet & 3 & 0.0106 & 746.8202 & 0.0116 &749.3610  \\ 
  VGG19 & 3 &  0.0106 & 745.9383 & 0.0116 &749.3610\\
 
 \hline
\end{tabular}
\caption{Performace of models using pretrained models ; Inception,VGG19, MobileNetv2 and ResNet}
\label{pretrained}
\end{table}

\section{ Performance of pre-trained models with multi frames}
As seen in table \ref{pretrainedlate}, the late fusion models using the pre-trained models; Inception, VGG19,  MobileNetv2 and ResNet performed much better than their equivalents single frame models. This better performance could be a mix of more information from the additional frame or the fact that the sgd optimization used for all the models with a momentum of 0.9, a decay of 0.0005 and a learning rate of 0.1, was a better optimization method for training.  All models also overfitted with some worse than others.  This could breed the argument that maybe some architectures are more vulnerable to overfitting than others. 

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
  \multicolumn{2}{|c|}{ } &
 \multicolumn{2}{|c|}{Training } &
 \multicolumn{2}{|c|}{validation} \\
 \hline
 model & epoch & accuracy & loss & accuracy & loss \\ 
  \hline
   Inception & 3 & 0.6900 & 2.9084 & 0.2297 &33.1893 \\ 
    MobileNetv2 & 3 & 0.8830  & 1.1337 & 0.2361 & 19.5433\\
  ResNet & 3 & 0.9298  & 0.7305 & 0.0095 & 24.4215   \\ 
 VGG19 & 3 & 0.3210 & 3.6636 & 0.0098 & 16.9945 \\
 \hline
\end{tabular}
\caption{Performance of late models using pretrained models ; Inception,VGG19, MobileNetv2 and ResNet}
\label{pretrainedlate}
\end{table}

\section{ Data Augmentation}
As seen in table \ref{dataAug}, the data augmentation did not make much of a difference to the performance of the single frame model. In fact, the results in a worse performance probability due to the loss of information in the images from the resizing and cropping.  This is not to say data augmentation is not beneficial, but rather more forms need to be explored for this dataset and task. 

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
  \multicolumn{2}{|c|}{ } &
 \multicolumn{2}{|c|}{Training } &
 \multicolumn{2}{|c|}{validation} \\
 \hline
 model & epoch & accuracy & loss & accuracy & loss \\ 
  \hline
   Inception & 20 & 0.6972 & 1.1184 & 0.1065 &6.0586 \\ 
 \hline
\end{tabular}
\caption{Pretrained models 1 epooch results}
\label{dataAug}
\end{table}

\section{ Computation}
On the UCL clusters, the use of different computing resources was explored, such as the used of RAM and the use of GPU vers CPU. Overall, for most models, the speed per each epoch increased significantly when going from 8GB to 128GB of RAM and when using a GPU versus without. 
 