@Inbook{Castellano2008,
author="Castellano, Ginevra
and Kessous, Loic
and Caridakis, George",
editor="Peter, Christian
and Beale, Russell",
title="Emotion Recognition through Multiple Modalities: Face, Body Gesture, Speech",
bookTitle="Affect and Emotion in Human-Computer Interaction: From Theory to Applications",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="92--103",
abstract="In this paper we present a multimodal approach for the recognition of eight emotions. Our approach integrates information from facial expressions, body movement and gestures and speech. We trained and tested a model with a Bayesian classifier, using a multimodal corpus with eight emotions and ten subjects. Firstly, individual classifiers were trained for each modality. Next, data were fused at the feature level and the decision level. Fusing the multimodal data resulted in a large increase in the recognition rates in comparison with the unimodal systems: the multimodal approach gave an improvement of more than 10{\%} when compared to the most successful unimodal system. Further, the fusion performed at the feature level provided better results than the one performed at the decision level.",
isbn="978-3-540-85099-1",
doi="10.1007/978-3-540-85099-1_8",
url="https://doi.org/10.1007/978-3-540-85099-1_8"
}


@article{CHEN2013175,
title = "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
journal = "Image and Vision Computing",
volume = "31",
number = "2",
pages = "175 - 185",
year = "2013",
note = "Affect Analysis In Continuous Input",
issn = "0262-8856",
doi = "https://doi.org/10.1016/j.imavis.2012.06.014",
url = "http://www.sciencedirect.com/science/article/pii/S0262885612001023",
author = "Shizhi Chen and YingLi Tian and Qingshan Liu and Dimitris N. Metaxas",
keywords = "Affect recognition, Facial feature, Body gesture, MHI-HOG, Image-HOG",
abstract = "Recently, recognizing affects from both face and body gestures attracts more attentions. However, it still lacks of efficient and effective features to describe the dynamics of face and gestures for real-time automatic affect recognition. In this paper, we combine both local motion and appearance feature in a novel framework to model the temporal dynamics of face and body gesture. The proposed framework employs MHI-HOG and Image-HOG features through temporal normalization or bag of words to capture motion and appearance information. The MHI-HOG stands for Histogram of Oriented Gradients (HOG) on the Motion History Image (MHI). It captures motion direction and speed of a region of interest as an expression evolves over the time. The Image-HOG captures the appearance information of the corresponding region of interest. The temporal normalization method explicitly solves the time resolution issue in the video-based affect recognition. To implicitly model local temporal dynamics of an expression, we further propose a bag of words (BOW) based representation for both MHI-HOG and Image-HOG features. Experimental results demonstrate promising performance as compared with the state-of-the-art. Significant improvement of recognition accuracy is achieved as compared with the frame-based approach that does not consider the underlying temporal dynamics."
}


@INPROCEEDINGS{5597316, 
author={S. {Singh} and S. A. {Velastin} and H. {Ragheb}}, 
booktitle={2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance}, 
title={MuHAVi: A Multicamera Human Action Video Dataset for the Evaluation of Action Recognition Methods}, 
year={2010}, 
volume={}, 
number={}, 
pages={48-55}, 
keywords={image recognition;video cameras;multicamera;human action video dataset;human action recognition method;multiaction dataset;Humans;Cameras;Feature extraction;Classification algorithms;Pixel;Training data;Training}, 
doi={10.1109/AVSS.2010.63}, 
ISSN={}, 
month={Aug},}

@article{SUN201836,
title = "Affect recognition from facial movements and body gestures by hierarchical deep spatio-temporal features and fusion strategy",
journal = "Neural Networks",
volume = "105",
pages = "36 - 51",
year = "2018",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2017.11.021",
url = "http://www.sciencedirect.com/science/article/pii/S0893608017302848",
author = "Bo Sun and Siming Cao and Jun He and Lejun Yu",
keywords = "Affect recognition, Deep learning, Convolutional neural network, Bilateral long short-term memory recurrent neural network, Deep spatio-temporal hierarchical feature, Multi-modal feature fusion strategy",
abstract = "Affect presentation is periodic and multi-modal, such as through facial movements, body gestures, and so on. Studies have shown that temporal selection and multi-modal combinations may benefit affect recognition. In this article, we therefore propose a spatio-temporal fusion model that extracts spatio-temporal hierarchical features based on select expressive components. In addition, a multi-modal hierarchical fusion strategy is presented. Our model learns the spatio-temporal hierarchical features from videos by a proposed deep network, which combines a convolutional neural networks (CNN), bilateral long short-term memory recurrent neural networks (BLSTM-RNN) with principal component analysis (PCA). Our approach handles each video as a “video sentence.” It first obtains a skeleton with the temporal selection process and then segments key words with a certain sliding window. Finally, it obtains the features with a deep network comprised of a video-skeleton and video-words. Our model combines the feature level and decision level fusion for fusing the multi-modal information. Experimental results showed that our model improved the multi-modal affect recognition accuracy rate from 95.13% in existing literature to 99.57% on a face and body (FABO) database, our results have been increased by 4.44%, and it obtained a macro average accuracy (MAA) up to 99.71%."
}

@INPROCEEDINGS{8578572, 
author={M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 
year={2018}, 
volume={}, 
number={}, 
pages={4510-4520}, 
keywords={convolution;image segmentation;mobile computing;object detection;mobile models;mobile semantic segmentation models;Mobile DeepLabv3;inverted residual structure;bottleneck layers;intermediate expansion layer;lightweight depthwise convolutions;narrow layers;COCO object detection;MobileNetV2;inverted residuals;linear bottlenecks;mobile architecture;Manifolds;Neural networks;Computer architecture;Standards;Computational modeling;Task analysis}, 
doi={10.1109/CVPR.2018.00474}, 
ISSN={2575-7075}, 
month={June},}


@article{CHEN2013175,
title = "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
journal = "Image and Vision Computing",
volume = "31",
number = "2",
pages = "175 - 185",
year = "2013",
note = "Affect Analysis In Continuous Input",
issn = "0262-8856",
doi = "https://doi.org/10.1016/j.imavis.2012.06.014",
url = "http://www.sciencedirect.com/science/article/pii/S0262885612001023",
author = "Shizhi Chen and YingLi Tian and Qingshan Liu and Dimitris N. Metaxas",
keywords = "Affect recognition, Facial feature, Body gesture, MHI-HOG, Image-HOG",
abstract = "Recently, recognizing affects from both face and body gestures attracts more attentions. However, it still lacks of efficient and effective features to describe the dynamics of face and gestures for real-time automatic affect recognition. In this paper, we combine both local motion and appearance feature in a novel framework to model the temporal dynamics of face and body gesture. The proposed framework employs MHI-HOG and Image-HOG features through temporal normalization or bag of words to capture motion and appearance information. The MHI-HOG stands for Histogram of Oriented Gradients (HOG) on the Motion History Image (MHI). It captures motion direction and speed of a region of interest as an expression evolves over the time. The Image-HOG captures the appearance information of the corresponding region of interest. The temporal normalization method explicitly solves the time resolution issue in the video-based affect recognition. To implicitly model local temporal dynamics of an expression, we further propose a bag of words (BOW) based representation for both MHI-HOG and Image-HOG features. Experimental results demonstrate promising performance as compared with the state-of-the-art. Significant improvement of recognition accuracy is achieved as compared with the frame-based approach that does not consider the underlying temporal dynamics."
}

@inproceedings{KarpathyCVPR14,
  title     = {Large-scale Video Classification with Convolutional Neural Networks},
  author    = {Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei},
  year      = {2014},
  booktitle = {CVPR}
}