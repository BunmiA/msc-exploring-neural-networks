@Inbook{Castellano2008,
  author="Castellano, Ginevra
and Kessous, Loic
and Caridakis, George",
  editor="Peter, Christian
and Beale, Russell",
  title="Emotion Recognition through Multiple Modalities: Face, Body Gesture, Speech",
  bookTitle="Affect and Emotion in Human-Computer Interaction: From Theory to Applications",
  year="2008",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="92--103",
  abstract="In this paper we present a multimodal approach for the recognition of eight emotions. Our approach integrates information from facial expressions, body movement and gestures and speech. We trained and tested a model with a Bayesian classifier, using a multimodal corpus with eight emotions and ten subjects. Firstly, individual classifiers were trained for each modality. Next, data were fused at the feature level and the decision level. Fusing the multimodal data resulted in a large increase in the recognition rates in comparison with the unimodal systems: the multimodal approach gave an improvement of more than 10{\%} when compared to the most successful unimodal system. Further, the fusion performed at the feature level provided better results than the one performed at the decision level.",
  isbn="978-3-540-85099-1",
  doi="10.1007/978-3-540-85099-1_8",
  url="https://doi.org/10.1007/978-3-540-85099-1_8"
}


@article{CHEN2013175,
  title = "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
  journal = "Image and Vision Computing",
  volume = "31",
  number = "2",
  pages = "175 - 185",
  year = "2013",
  note = "Affect Analysis In Continuous Input",
  issn = "0262-8856",
  doi = "https://doi.org/10.1016/j.imavis.2012.06.014",
  url = "http://www.sciencedirect.com/science/article/pii/S0262885612001023",
  author = "Shizhi Chen and YingLi Tian and Qingshan Liu and Dimitris N. Metaxas",
  keywords = "Affect recognition, Facial feature, Body gesture, MHI-HOG, Image-HOG",
  abstract = "Recently, recognizing affects from both face and body gestures attracts more attentions. However, it still lacks of efficient and effective features to describe the dynamics of face and gestures for real-time automatic affect recognition. In this paper, we combine both local motion and appearance feature in a novel framework to model the temporal dynamics of face and body gesture. The proposed framework employs MHI-HOG and Image-HOG features through temporal normalization or bag of words to capture motion and appearance information. The MHI-HOG stands for Histogram of Oriented Gradients (HOG) on the Motion History Image (MHI). It captures motion direction and speed of a region of interest as an expression evolves over the time. The Image-HOG captures the appearance information of the corresponding region of interest. The temporal normalization method explicitly solves the time resolution issue in the video-based affect recognition. To implicitly model local temporal dynamics of an expression, we further propose a bag of words (BOW) based representation for both MHI-HOG and Image-HOG features. Experimental results demonstrate promising performance as compared with the state-of-the-art. Significant improvement of recognition accuracy is achieved as compared with the frame-based approach that does not consider the underlying temporal dynamics."
}


@INPROCEEDINGS{5597316,
author={S. {Singh} and S. A. {Velastin} and H. {Ragheb}},
booktitle={2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance},
title={MuHAVi: A Multicamera Human Action Video Dataset for the Evaluation of Action Recognition Methods},
year={2010},
volume={},
number={},
pages={48-55},
keywords={image recognition;video cameras;multicamera;human action video dataset;human action recognition method;multiaction dataset;Humans;Cameras;Feature extraction;Classification algorithms;Pixel;Training data;Training},
doi={10.1109/AVSS.2010.63},
ISSN={},
month={Aug},}

@article{SUN201836,
title = "Affect recognition from facial movements and body gestures by hierarchical deep spatio-temporal features and fusion strategy",
journal = "Neural Networks",
volume = "105",
pages = "36 - 51",
year = "2018",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2017.11.021",
url = "http://www.sciencedirect.com/science/article/pii/S0893608017302848",
author = "Bo Sun and Siming Cao and Jun He and Lejun Yu",
keywords = "Affect recognition, Deep learning, Convolutional neural network, Bilateral long short-term memory recurrent neural network, Deep spatio-temporal hierarchical feature, Multi-modal feature fusion strategy",
abstract = "Affect presentation is periodic and multi-modal, such as through facial movements, body gestures, and so on. Studies have shown that temporal selection and multi-modal combinations may benefit affect recognition. In this article, we therefore propose a spatio-temporal fusion model that extracts spatio-temporal hierarchical features based on select expressive components. In addition, a multi-modal hierarchical fusion strategy is presented. Our model learns the spatio-temporal hierarchical features from videos by a proposed deep network, which combines a convolutional neural networks (CNN), bilateral long short-term memory recurrent neural networks (BLSTM-RNN) with principal component analysis (PCA). Our approach handles each video as a “video sentence.” It first obtains a skeleton with the temporal selection process and then segments key words with a certain sliding window. Finally, it obtains the features with a deep network comprised of a video-skeleton and video-words. Our model combines the feature level and decision level fusion for fusing the multi-modal information. Experimental results showed that our model improved the multi-modal affect recognition accuracy rate from 95.13% in existing literature to 99.57% on a face and body (FABO) database, our results have been increased by 4.44%, and it obtained a macro average accuracy (MAA) up to 99.71%."
}

@INPROCEEDINGS{8578572,
author={M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
year={2018},
volume={},
number={},
pages={4510-4520},
keywords={convolution;image segmentation;mobile computing;object detection;mobile models;mobile semantic segmentation models;Mobile DeepLabv3;inverted residual structure;bottleneck layers;intermediate expansion layer;lightweight depthwise convolutions;narrow layers;COCO object detection;MobileNetV2;inverted residuals;linear bottlenecks;mobile architecture;Manifolds;Neural networks;Computer architecture;Standards;Computational modeling;Task analysis},
doi={10.1109/CVPR.2018.00474},
ISSN={2575-7075},
month={June},}


@article{CHEN2013175,
title = "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
journal = "Image and Vision Computing",
volume = "31",
number = "2",
pages = "175 - 185",
year = "2013",
note = "Affect Analysis In Continuous Input",
issn = "0262-8856",
doi = "https://doi.org/10.1016/j.imavis.2012.06.014",
url = "http://www.sciencedirect.com/science/article/pii/S0262885612001023",
author = "Shizhi Chen and YingLi Tian and Qingshan Liu and Dimitris N. Metaxas",
keywords = "Affect recognition, Facial feature, Body gesture, MHI-HOG, Image-HOG",
abstract = "Recently, recognizing affects from both face and body gestures attracts more attentions. However, it still lacks of efficient and effective features to describe the dynamics of face and gestures for real-time automatic affect recognition. In this paper, we combine both local motion and appearance feature in a novel framework to model the temporal dynamics of face and body gesture. The proposed framework employs MHI-HOG and Image-HOG features through temporal normalization or bag of words to capture motion and appearance information. The MHI-HOG stands for Histogram of Oriented Gradients (HOG) on the Motion History Image (MHI). It captures motion direction and speed of a region of interest as an expression evolves over the time. The Image-HOG captures the appearance information of the corresponding region of interest. The temporal normalization method explicitly solves the time resolution issue in the video-based affect recognition. To implicitly model local temporal dynamics of an expression, we further propose a bag of words (BOW) based representation for both MHI-HOG and Image-HOG features. Experimental results demonstrate promising performance as compared with the state-of-the-art. Significant improvement of recognition accuracy is achieved as compared with the frame-based approach that does not consider the underlying temporal dynamics."
}

@inproceedings{KarpathyCVPR14,
title     = {Large-scale Video Classification with Convolutional Neural Networks},
author    = {Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei},
year      = {2014},
booktitle = {CVPR}
}

@article{MISHKIN201711,
title = "Systematic evaluation of convolution neural network advances on the Imagenet",
journal = "Computer Vision and Image Understanding",
volume = "161",
pages = "11 - 19",
year = "2017",
issn = "1077-3142",
doi = "https://doi.org/10.1016/j.cviu.2017.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S1077314217300814",
author = "Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas",
keywords = "CNN, Benchmark, Non-linearity, Pooling, ImageNet",
abstract = "The paper systematically studies the impact of a range of recent advances in convolution neural network (CNN) architectures and learning methods on the object categorization (ILSVRC) problem. The evaluation tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is greater than the observed improvement when all modifications are introduced, but the “deficit” is small suggesting independence of their benefits. We show that the use of 128 × 128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images."
}


@inproceedings{JiaDeng2009IAlh,
issn = {10636919},
abstract = {<p>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</p>},
pages = {248--255},
publisher = {IEEE},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
isbn = {9781424439928},
year = {2009},
title = {ImageNet: A large-scale hierarchical image database},
language = {eng},
author = {Jia Deng and Wei Dong and Socher, Richard and Li-Jia Li and Kai Li and Li Fei-Fei},
keywords = {Large-Scale Systems ; Image Databases ; Explosions ; Internet ; Robustness ; Information Retrieval ; Image Retrieval ; Multimedia Databases ; Ontologies ; Spine ; Applied Sciences ; Computer Science},
}

@article{MISHKIN201711,
title = "Systematic evaluation of convolution neural network advances on the Imagenet",
journal = "Computer Vision and Image Understanding",
volume = "161",
pages = "11 - 19",
year = "2017",
issn = "1077-3142",
doi = "https://doi.org/10.1016/j.cviu.2017.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S1077314217300814",
author = "Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas",
keywords = "CNN, Benchmark, Non-linearity, Pooling, ImageNet",
abstract = "The paper systematically studies the impact of a range of recent advances in convolution neural network (CNN) architectures and learning methods on the object categorization (ILSVRC) problem. The evaluation tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is greater than the observed improvement when all modifications are introduced, but the “deficit” is small suggesting independence of their benefits. We show that the use of 128 × 128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images."
}

@misc{simonyan2014deep,
title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
author={Karen Simonyan and Andrew Zisserman},
year={2014},
eprint={1409.1556},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@article{He_2016,
title={Deep Residual Learning for Image Recognition},
ISBN={9781467388511},
url={http://dx.doi.org/10.1109/CVPR.2016.90},
DOI={10.1109/cvpr.2016.90},
journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
publisher={IEEE},
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year={2016},
month={Jun}
}

@article{Kalchbrenner_2014,
title={A Convolutional Neural Network for Modelling Sentences},
url={http://dx.doi.org/10.3115/v1/P14-1062},
DOI={10.3115/v1/p14-1062},
journal={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
publisher={Association for Computational Linguistics},
author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
year={2014}
}


@misc{knyazev2017convolutional,
title={Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video},
author={Boris Knyazev and Roman Shvetsov and Natalia Efremova and Artem Kuharenko},
year={2017},
eprint={1711.04598},
archivePrefix={arXiv},
primaryClass={cs.CV}
}


@misc{soomro2012ucf101,
title={UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},
author={Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
year={2012},
eprint={1212.0402},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{ROBINSON20071631,
title = "Explaining brightness illusions using spatial filtering and local response normalization",
journal = "Vision Research",
volume = "47",
number = "12",
pages = "1631 - 1644",
year = "2007",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2007.02.017",
url = "http://www.sciencedirect.com/science/article/pii/S0042698907000648",
author = "Alan E. Robinson and Paul S. Hammon and Virginia R. de Sa",
keywords = "Brightness, White’s effect, Contrast, Computational modeling",
abstract = "We introduce two new low-level computational models of brightness perception that account for a wide range of brightness illusions, including many variations on White’s Effect [Perception, 8, 1979, 413]. Our models extend Blakeslee and McCourt’s ODOG model [Vision Research, 39, 1999, 4361], which combines multiscale oriented difference-of-Gaussian filters and response normalization. We extend the response normalization to be more neurally plausible by constraining normalization to nearby receptive fields (models 1 and 2) and spatial frequencies (model 2), and show that both of these changes increase the effectiveness of the models at predicting brightness illusions."
}


@misc{ioffe2015batch,
title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
author={Sergey Ioffe and Christian Szegedy},
year={2015},
eprint={1502.03167},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@article{Sandler_2018,
title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
ISBN={9781538664209},
url={http://dx.doi.org/10.1109/cvpr.2018.00474},
DOI={10.1109/cvpr.2018.00474},
journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
publisher={IEEE},
author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
year={2018},
month={Jun}
}