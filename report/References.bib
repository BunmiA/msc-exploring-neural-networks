@Inbook{Castellano2008,
  author="Castellano, Ginevra
and Kessous, Loic
and Caridakis, George",
  editor="Peter, Christian
and Beale, Russell",
  title="Emotion Recognition through Multiple Modalities: Face, Body Gesture, Speech",
  bookTitle="Affect and Emotion in Human-Computer Interaction: From Theory to Applications",
  year="2008",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="92--103",
  abstract="In this paper we present a multimodal approach for the recognition of eight emotions. Our approach integrates information from facial expressions, body movement and gestures and speech. We trained and tested a model with a Bayesian classifier, using a multimodal corpus with eight emotions and ten subjects. Firstly, individual classifiers were trained for each modality. Next, data were fused at the feature level and the decision level. Fusing the multimodal data resulted in a large increase in the recognition rates in comparison with the unimodal systems: the multimodal approach gave an improvement of more than 10{\%} when compared to the most successful unimodal system. Further, the fusion performed at the feature level provided better results than the one performed at the decision level.",
  isbn="978-3-540-85099-1",
  doi="10.1007/978-3-540-85099-1_8",
  url="https://doi.org/10.1007/978-3-540-85099-1_8"
}



@article{CHEN2013175,
  title = "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
  journal = "Image and Vision Computing",
  volume = "31",
  number = "2",
  pages = "175 - 185",
  year = "2013",
  note = "Affect Analysis In Continuous Input",
  issn = "0262-8856",
  doi = "https://doi.org/10.1016/j.imavis.2012.06.014",
  url = "http://www.sciencedirect.com/science/article/pii/S0262885612001023",
  author = "Shizhi Chen and YingLi Tian and Qingshan Liu and Dimitris N. Metaxas",
  keywords = "Affect recognition, Facial feature, Body gesture, MHI-HOG, Image-HOG",
  abstract = "Recently, recognizing affects from both face and body gestures attracts more attentions. However, it still lacks of efficient and effective features to describe the dynamics of face and gestures for real-time automatic affect recognition. In this paper, we combine both local motion and appearance feature in a novel framework to model the temporal dynamics of face and body gesture. The proposed framework employs MHI-HOG and Image-HOG features through temporal normalization or bag of words to capture motion and appearance information. The MHI-HOG stands for Histogram of Oriented Gradients (HOG) on the Motion History Image (MHI). It captures motion direction and speed of a region of interest as an expression evolves over the time. The Image-HOG captures the appearance information of the corresponding region of interest. The temporal normalization method explicitly solves the time resolution issue in the video-based affect recognition. To implicitly model local temporal dynamics of an expression, we further propose a bag of words (BOW) based representation for both MHI-HOG and Image-HOG features. Experimental results demonstrate promising performance as compared with the state-of-the-art. Significant improvement of recognition accuracy is achieved as compared with the frame-based approach that does not consider the underlying temporal dynamics."
}


@INPROCEEDINGS{5597316,
author={S. {Singh} and S. A. {Velastin} and H. {Ragheb}},
booktitle={2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance},
title={MuHAVi: A Multicamera Human Action Video Dataset for the Evaluation of Action Recognition Methods},
year={2010},
volume={},
number={},
pages={48-55},
keywords={image recognition;video cameras;multicamera;human action video dataset;human action recognition method;multiaction dataset;Humans;Cameras;Feature extraction;Classification algorithms;Pixel;Training data;Training},
doi={10.1109/AVSS.2010.63},
ISSN={},
month={Aug},}

@article{SUN201836,
title = "Affect recognition from facial movements and body gestures by hierarchical deep spatio-temporal features and fusion strategy",
journal = "Neural Networks",
volume = "105",
pages = "36 - 51",
year = "2018",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2017.11.021",
url = "http://www.sciencedirect.com/science/article/pii/S0893608017302848",
author = "Bo Sun and Siming Cao and Jun He and Lejun Yu",
keywords = "Affect recognition, Deep learning, Convolutional neural network, Bilateral long short-term memory recurrent neural network, Deep spatio-temporal hierarchical feature, Multi-modal feature fusion strategy",
abstract = "Affect presentation is periodic and multi-modal, such as through facial movements, body gestures, and so on. Studies have shown that temporal selection and multi-modal combinations may benefit affect recognition. In this article, we therefore propose a spatio-temporal fusion model that extracts spatio-temporal hierarchical features based on select expressive components. In addition, a multi-modal hierarchical fusion strategy is presented. Our model learns the spatio-temporal hierarchical features from videos by a proposed deep network, which combines a convolutional neural networks (CNN), bilateral long short-term memory recurrent neural networks (BLSTM-RNN) with principal component analysis (PCA). Our approach handles each video as a “video sentence.” It first obtains a skeleton with the temporal selection process and then segments key words with a certain sliding window. Finally, it obtains the features with a deep network comprised of a video-skeleton and video-words. Our model combines the feature level and decision level fusion for fusing the multi-modal information. Experimental results showed that our model improved the multi-modal affect recognition accuracy rate from 95.13% in existing literature to 99.57% on a face and body (FABO) database, our results have been increased by 4.44%, and it obtained a macro average accuracy (MAA) up to 99.71%."
}

@INPROCEEDINGS{8578572,
author={M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
year={2018},
volume={},
number={},
pages={4510-4520},
keywords={convolution;image segmentation;mobile computing;object detection;mobile models;mobile semantic segmentation models;Mobile DeepLabv3;inverted residual structure;bottleneck layers;intermediate expansion layer;lightweight depthwise convolutions;narrow layers;COCO object detection;MobileNetV2;inverted residuals;linear bottlenecks;mobile architecture;Manifolds;Neural networks;Computer architecture;Standards;Computational modeling;Task analysis},
doi={10.1109/CVPR.2018.00474},
ISSN={2575-7075},
month={June},}


@article{CHEN2013175,
title = "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
journal = "Image and Vision Computing",
volume = "31",
number = "2",
pages = "175 - 185",
year = "2013",
note = "Affect Analysis In Continuous Input",
issn = "0262-8856",
doi = "https://doi.org/10.1016/j.imavis.2012.06.014",
url = "http://www.sciencedirect.com/science/article/pii/S0262885612001023",
author = "Shizhi Chen and YingLi Tian and Qingshan Liu and Dimitris N. Metaxas",
keywords = "Affect recognition, Facial feature, Body gesture, MHI-HOG, Image-HOG",
abstract = "Recently, recognizing affects from both face and body gestures attracts more attentions. However, it still lacks of efficient and effective features to describe the dynamics of face and gestures for real-time automatic affect recognition. In this paper, we combine both local motion and appearance feature in a novel framework to model the temporal dynamics of face and body gesture. The proposed framework employs MHI-HOG and Image-HOG features through temporal normalization or bag of words to capture motion and appearance information. The MHI-HOG stands for Histogram of Oriented Gradients (HOG) on the Motion History Image (MHI). It captures motion direction and speed of a region of interest as an expression evolves over the time. The Image-HOG captures the appearance information of the corresponding region of interest. The temporal normalization method explicitly solves the time resolution issue in the video-based affect recognition. To implicitly model local temporal dynamics of an expression, we further propose a bag of words (BOW) based representation for both MHI-HOG and Image-HOG features. Experimental results demonstrate promising performance as compared with the state-of-the-art. Significant improvement of recognition accuracy is achieved as compared with the frame-based approach that does not consider the underlying temporal dynamics."
}

@inproceedings{KarpathyCVPR14,
title     = {Large-scale Video Classification with Convolutional Neural Networks},
author    = {Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei},
year      = {2014},
booktitle = {CVPR}
}

@article{MISHKIN201711,
title = "Systematic evaluation of convolution neural network advances on the Imagenet",
journal = "Computer Vision and Image Understanding",
volume = "161",
pages = "11 - 19",
year = "2017",
issn = "1077-3142",
doi = "https://doi.org/10.1016/j.cviu.2017.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S1077314217300814",
author = "Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas",
keywords = "CNN, Benchmark, Non-linearity, Pooling, ImageNet",
abstract = "The paper systematically studies the impact of a range of recent advances in convolution neural network (CNN) architectures and learning methods on the object categorization (ILSVRC) problem. The evaluation tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is greater than the observed improvement when all modifications are introduced, but the “deficit” is small suggesting independence of their benefits. We show that the use of 128 × 128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images."
}


@inproceedings{JiaDeng2009IAlh,
issn = {10636919},
abstract = {<p>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</p>},
pages = {248--255},
publisher = {IEEE},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
isbn = {9781424439928},
year = {2009},
title = {ImageNet: A large-scale hierarchical image database},
language = {eng},
author = {Jia Deng and Wei Dong and Socher, Richard and Li-Jia Li and Kai Li and Li Fei-Fei},
keywords = {Large-Scale Systems ; Image Databases ; Explosions ; Internet ; Robustness ; Information Retrieval ; Image Retrieval ; Multimedia Databases ; Ontologies ; Spine ; Applied Sciences ; Computer Science},
}

@article{MISHKIN201711,
title = "Systematic evaluation of convolution neural network advances on the Imagenet",
journal = "Computer Vision and Image Understanding",
volume = "161",
pages = "11 - 19",
year = "2017",
issn = "1077-3142",
doi = "https://doi.org/10.1016/j.cviu.2017.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S1077314217300814",
author = "Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas",
keywords = "CNN, Benchmark, Non-linearity, Pooling, ImageNet",
abstract = "The paper systematically studies the impact of a range of recent advances in convolution neural network (CNN) architectures and learning methods on the object categorization (ILSVRC) problem. The evaluation tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is greater than the observed improvement when all modifications are introduced, but the “deficit” is small suggesting independence of their benefits. We show that the use of 128 × 128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images."
}

@misc{simonyan2014deep,
title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
author={Karen Simonyan and Andrew Zisserman},
year={2014},
eprint={1409.1556},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@article{He_2016,
title={Deep Residual Learning for Image Recognition},
ISBN={9781467388511},
url={http://dx.doi.org/10.1109/CVPR.2016.90},
DOI={10.1109/cvpr.2016.90},
journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
publisher={IEEE},
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year={2016},
month={Jun}
}

@article{Kalchbrenner_2014,
title={A Convolutional Neural Network for Modelling Sentences},
url={http://dx.doi.org/10.3115/v1/P14-1062},
DOI={10.3115/v1/p14-1062},
journal={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
publisher={Association for Computational Linguistics},
author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
year={2014}
}


@misc{knyazev2017convolutional,
title={Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video},
author={Boris Knyazev and Roman Shvetsov and Natalia Efremova and Artem Kuharenko},
year={2017},
eprint={1711.04598},
archivePrefix={arXiv},
primaryClass={cs.CV}
}


@misc{soomro2012ucf101,
title={UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},
author={Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
year={2012},
eprint={1212.0402},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{ROBINSON20071631,
title = "Explaining brightness illusions using spatial filtering and local response normalization",
journal = "Vision Research",
volume = "47",
number = "12",
pages = "1631 - 1644",
year = "2007",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2007.02.017",
url = "http://www.sciencedirect.com/science/article/pii/S0042698907000648",
author = "Alan E. Robinson and Paul S. Hammon and Virginia R. de Sa",
keywords = "Brightness, White’s effect, Contrast, Computational modeling",
abstract = "We introduce two new low-level computational models of brightness perception that account for a wide range of brightness illusions, including many variations on White’s Effect [Perception, 8, 1979, 413]. Our models extend Blakeslee and McCourt’s ODOG model [Vision Research, 39, 1999, 4361], which combines multiscale oriented difference-of-Gaussian filters and response normalization. We extend the response normalization to be more neurally plausible by constraining normalization to nearby receptive fields (models 1 and 2) and spatial frequencies (model 2), and show that both of these changes increase the effectiveness of the models at predicting brightness illusions."
}


@misc{ioffe2015batch,
title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
author={Sergey Ioffe and Christian Szegedy},
year={2015},
eprint={1502.03167},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@article{Sandler_2018,
   title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
   ISBN={9781538664209},
   url={http://dx.doi.org/10.1109/cvpr.2018.00474},
   DOI={10.1109/cvpr.2018.00474},
   journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
   publisher={IEEE},
   author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
   year={2018},
   month={Jun}
}

@inproceedings{45381,
title	= {TensorFlow: A system for large-scale machine learning},
author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2016},
URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages	= {265--283}
}


@article{DBLPjournalscorrBuitinckLBPMGNPGGLVJHV13,
  author    = {Lars Buitinck and
               Gilles Louppe and
               Mathieu Blondel and
               Fabian Pedregosa and
               Andreas Mueller and
               Olivier Grisel and
               Vlad Niculae and
               Peter Prettenhofer and
               Alexandre Gramfort and
               Jaques Grobler and
               Robert Layton and
               Jake VanderPlas and
               Arnaud Joly and
               Brian Holt and
               Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  journal   = {CoRR},
  volume    = {abs/1309.0238},
  year      = {2013},
  url       = {http://arxiv.org/abs/1309.0238},
  archivePrefix = {arXiv},
  eprint    = {1309.0238},
  timestamp = {Mon, 13 Aug 2018 16:49:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BuitinckLBPMGNPGGLVJHV13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Article{Wang2019,
author="Wang, Zhaobin
and Liu, Ke
and Li, Jian
and Zhu, Ying
and Zhang, Yaonan",
title="Various Frameworks and Libraries of Machine Learning and Deep Learning: A Survey",
journal="Archives of Computational Methods in Engineering",
year="2019",
month="Feb",
day="01",
abstract="With the rapid development of deep learning in various fields, the big companies and research teams have developed independent and unique tools. This paper collects 18 common deep learning frameworks and libraries (Caffe, Caffe2, Tensorflow, Theano include Keras Lasagnes and Blocks, MXNet, CNTK, Torch, PyTorch, Pylearn2, Scikit-learn, Matlab include MatconvNet Matlab deep learning and Deep learning tool box, Chainer, Deeplearning4j) and introduces a large number of benchmarking data. In addition, we give the overall score of the current eight mainstream deep learning frameworks from six aspects (model design ability, interface property, deployment ability, performance, framework design and prospects for development). Based on our overview, the deep learning researchers can choose the appropriate development tools according to the evaluation criteria. By summarizing the 18 deep learning frameworks and libraries, we have found that most of the deep learning tools are moving closer to the mobile terminal, and the role of ASICs is gradually emerging. It is believed that the future deep learning applications will be inseparable from the ASIC support.",
issn="1886-1784",
doi="10.1007/s11831-018-09312-w",
url="https://doi.org/10.1007/s11831-018-09312-w"
}

@Article{Nguyen2019,
author="Nguyen, Giang
and Dlugolinsky, Stefan
and Bob{\'a}k, Martin
and Tran, Viet
and L{\'o}pez Garc{\'i}a, {\'A}lvaro
and Heredia, Ignacio
and Mal{\'i}k, Peter
and Hluch{\'y}, Ladislav",
title="Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey",
journal="Artificial Intelligence Review",
year="2019",
month="Jun",
day="01",
volume="52",
number="1",
pages="77--124",
abstract="The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.",
issn="1573-7462",
doi="10.1007/s10462-018-09679-z",
url="https://doi.org/10.1007/s10462-018-09679-z"
}

@misc{goldsborough2016tour,
    title={A Tour of TensorFlow},
    author={Peter Goldsborough},
    year={2016},
    eprint={1610.01178},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{schrimpf2016i,
    title={Should I use TensorFlow},
    author={Martin Schrimpf},
    year={2016},
    eprint={1611.08903},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{doi:10.1162neco_a_00990,
author = {Rawat, Waseem and Wang, Zenghui},
title = {Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review},
journal = {Neural Computation},
volume = {29},
number = {9},
pages = {2352-2449},
year = {2017},
doi = {10.1162/neco\_a\_00990},
    note ={PMID: 28599112},

URL = { 
        https://doi.org/10.1162/neco_a_00990
    
},
eprint = { 
        https://doi.org/10.1162/neco_a_00990
    
}
,
    abstract = { Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges. }
}

@article{TomeD2016DCNN,
journal = {Signal Processing: Image Communication , 47 pp. 482-489. (2016)},
year = {2016},
title = {Deep Convolutional Neural Networks for pedestrian detection},
copyright = {open},
language = {eng},
author = {Tomè, D and Monti, F and Baroffio, L and Bondi, L and Tagliasacchi, M and Tubaro, S},
keywords = {Deep learning; Pedestrian detection; Convolutional Neural Networks; Optimization},
}

@article{PramerdorferChristopher2016FERu,
abstract = {The ability to recognize facial expressions automatically enables novel applications in human-computer interaction and other areas. Consequently, there has been active research in this field, with several recent works utilizing Convolutional Neural Networks (CNNs) for feature extraction and inference. These works differ significantly in terms of CNN architectures and other factors. Based on the reported results alone, the performance impact of these factors is unclear. In this paper, we review the state of the art in image-based facial expression recognition using CNNs and highlight algorithmic differences and their performance impact. On this basis, we identify existing bottlenecks and consequently directions for advancing this research field. Furthermore, we demonstrate that overcoming one of these bottlenecks - the comparatively basic architectures of the CNNs utilized in this field - leads to a substantial performance increase. By forming an ensemble of modern deep CNNs, we obtain a FER2013 test accuracy of 75.2%, outperforming previous works without requiring auxiliary training data or face registration.},
year = {2016},
title = {Facial Expression Recognition using Convolutional Neural Networks: State of the Art},
author = {Pramerdorfer, Christopher and Kampel, Martin},
keywords = {Computer Science - Computer Vision And Pattern Recognition},
}

@article{Abdel-HamidOssama2014CNNf,
issn = {2329-9290},
abstract = {<p>Recently, the hybrid deep neural network (DNN)-hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6%-10% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.</p>},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
pages = {1533--1545},
volume = {22},
publisher = {IEEE},
number = {10},
year = {2014},
title = {Convolutional Neural Networks for Speech Recognition},
language = {eng},
author = {Abdel-Hamid, Ossama and Mohamed, Abdel-Rahman and Hui Jiang and Li Deng and Penn, Gerald and Dong Yu},
keywords = {Convolution ; Hidden Markov Models ; Speech ; Speech Recognition ; Vectors ; Neural Networks ; Training ; Convolution ; Convolutional Neural Networks ; Limited Weight Sharing (Lws) Scheme ; Pooling ; Engineering},
}

@article{Szegedy_2016,
   title={Rethinking the Inception Architecture for Computer Vision},
   ISBN={9781467388511},
   url={http://dx.doi.org/10.1109/CVPR.2016.308},
   DOI={10.1109/cvpr.2016.308},
   journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
   year={2016},
   month={Jun}
}


@InProceedings{10.1007978-3-642-28493-9_34,
author="Duong, Dinh
and Dinh, Thang Ba
and Dinh, Tien
and Duong, Duc",
editor="Pan, Jeng-Shyang
and Chen, Shyi-Ming
and Nguyen, Ngoc Thanh",
title="Sports Video Classification Using Bag of Words Model",
booktitle="Intelligent Information and Database Systems",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="316--325",
abstract="We propose a novel approach classify different sports videos given their groups. First, the SURF descriptors in each key frames are extracted. Then they are used to form the visual word vocabulary (codebook) by using K-Means clustering algorithm. After that, the histogram of these visual words are computed and considered as a feature vector. Finally, we use SVM to train each classifier for each category. The classification result of the video is the production of the scores output from all of the key frames. An extensive experiment is performed on a diverse and challenging dataset of 600 sports video clips downloaded from Youtube with a total of more than 6000 minutes in length for 10 different kinds of sports.",
isbn="978-3-642-28493-9"
}

@Article{Uijlings2015,
author="Uijlings, J.
and Duta, I. C.
and Sangineto, E.
and Sebe, Nicu",
title="Video classification with Densely extracted HOG/HOF/MBH features: an evaluation of the accuracy/computational efficiency trade-off",
journal="International Journal of Multimedia Information Retrieval",
year="2015",
month="Mar",
day="01",
volume="4",
number="1",
pages="33--44",
abstract="The current state-of-the-art in video classification is based on Bag-of-Words using local visual descriptors. Most commonly these are histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary histograms (MBH) descriptors. While such approach is very powerful for classification, it is also computationally expensive. This paper addresses the problem of computational efficiency. Specifically: (1) We propose several speed-ups for densely sampled HOG, HOF and MBH descriptors and release Matlab code; (2) We investigate the trade-off between accuracy and computational efficiency of descriptors in terms of frame sampling rate and type of Optical Flow method; (3) We investigate the trade-off between accuracy and computational efficiency for computing the feature vocabulary, using and comparing most of the commonly adopted vector quantization techniques: {\$}{\$}k{\$}{\$}k-means, hierarchical {\$}{\$}k{\$}{\$}k-means, Random Forests, Fisher Vectors and VLAD.",
issn="2192-662X",
doi="10.1007/s13735-014-0069-5",
url="https://doi.org/10.1007/s13735-014-0069-5"
}


@misc{yang2017tensortrain,
    title={Tensor-Train Recurrent Neural Networks for Video Classification},
    author={Yinchong Yang and Denis Krompass and Volker Tresp},
    year={2017},
    eprint={1707.01786},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{Randles_2017,
   title={Using the Jupyter Notebook as a Tool for Open Science: An Empirical Study},
   ISBN={9781538638613},
   url={http://dx.doi.org/10.1109/jcdl.2017.7991618},
   DOI={10.1109/jcdl.2017.7991618},
   journal={2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
   publisher={IEEE},
   author={Randles, Bernadette M. and Pasquetto, Irene V. and Golshan, Milena S. and Borgman, Christine L.},
   year={2017},
   month={Jun}
}

@inproceedings{45619,
title	= {YouTube-8M: A Large-Scale Video Classification Benchmark},
author	= {Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and Apostol (Paul) Natsev and George Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},
year	= {2016},
URL	= {https://arxiv.org/pdf/1609.08675v1.pdf},
booktitle	= {arXiv:1609.08675}
}


@article{Lux:2019:OSC:3310195.3310202,
 author = {Lux, Mathias and Bertini, Marco},
 title = {Open Source Column: Deep Learning with Keras},
 journal = {SIGMultimedia Rec.},
 issue_date = {December 2018},
 volume = {10},
 number = {4},
 month = jan,
 year = {2019},
 issn = {1947-4598},
 pages = {7:7--7:7},
 articleno = {7},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/3310195.3310202},
 doi = {10.1145/3310195.3310202},
 acmid = {3310202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@misc{wang2019benchmarking,
    title={Benchmarking TPU, GPU, and CPU Platforms for Deep Learning},
    author={Yu Emma Wang and Gu-Yeon Wei and David Brooks},
    year={2019},
    eprint={1907.10701},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{ChadhaA2017VCWC,
journal = {IEEE Transactions on Circuits and Systems for Video Technology (2017) (In press).},
year = {2017},
title = {Video Classification With CNNs: Using The Codec As A Spatio-Temporal Activity Sensor},
copyright = {open},
language = {eng},
author = {Chadha, A and Abbas, A and Andreopoulos, Y},
keywords = {video coding;  classification;  deep learning;  Computer architecture;  Decoding;  Three-dimensional displays;  Two dimensional displays;  Training;  Complexity theory},
}



@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{howard2017mobilenets,
    title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
    author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
    year={2017},
    eprint={1704.04861},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{kingma2014adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ruder2016overview,
    title={An overview of gradient descent optimization algorithms},
    author={Sebastian Ruder},
    year={2016},
    eprint={1609.04747},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{projectGithub,
      author = "Olubunmi Aworant",
      title = "Project Github",
      month = "September",
      year = "2019",
      url = "https://github.com/BunmiA/msc-exploring-neural-networks"
}