\chapter{Coding}
\label{coding}
This section highlights some of the core items the code used to implement models. The code for all models and batch jobs has also been checked into a Github repository.  The details of the repository can be seen in \citep{projectGithub}.  
All of the code, begins by first importing the TensorFlow and TensorFlow dataset libraries.

\begin{lstlisting}[language=Python, caption=Importing tensorflow libaries]
    import tensorflow as tf
    import tensorflow_datasets as tfds
\end{lstlisting}

\section{Data Loading}
Using the TensorFlow dataset library allowed for easy loading of the UCF101 dataset as seen below. The library also provides a default recommended training and test dataset split. It also provides an information object detailing the label and dataset.

\begin{lstlisting}[language=Python, caption=Loading UCF101 dataset]
    ucf101_dataset, ucf101_info = tfds.load(name="ucf101", with_info=True)
   ucf101_train , ucf101_test = ucf101_dataset["train"], ucf101_dataset["test"]
\end{lstlisting}

\section{Model Implementation}
% todo talk about batches and shuffling

The Keras API provides a wrapper that allows for easy implementation of network layers, as seen in Figure \ref{fig:neuralnetwork}. This wrapper conceals the learning logic and allows the user to specify the layer type, input dimensions, activation function, number of nodes and other layer-specific parameters.
Most layers types needed were present in the Keras API. Most models were implemented using the Keras sequential model, which forms a neural network by linearly stacking the Keras layers as seen below. This is quite similar to how a neural network is formed, as seen in figure \ref{fig:neuralnetwork}.  The single frame model, for example, was implemented as a Keras sequential model by passing in the list of layers as seen in \ref{fig:k_models}. The Keras API also offers another implementation of networks which involves using it's add method to add in layers. This is very useful for more complex architectures such as the slow fusion layer. It uses the fact that a layer is a callable instance that can be called on a tensor and the return a tensor. This is very reminiscent of the inner working of a neural network where the output of one layer is the input of the next layer. Having callable layers allows for building multi-output and multi-input models like the slow and late fusion model.

\begin{lstlisting}[language=Python, caption=Single frame model implemetation, label=singleFrame]
    model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(96, (11,11), strides=3 , activation='relu', input_shape=(170, 170, 3)),

    l.MyLRNLayer(),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(256, (5,5), strides=1, activation='relu'),

    l.MyLRNLayer(),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(384, (3,3), strides=1, activation='relu'),

    tf.keras.layers.Conv2D(384, (3,3), strides=1, activation='relu'),
    tf.keras.layers.Conv2D(256, (3,3), strides=1, activation='relu'),

    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),

    tf.keras.layers.Dense(4096, activation='relu'),
    tf.keras.layers.Dense(4096, activation='relu'),
    tf.keras.layers.Dense(101, activation='softmax')
    ])
\end{lstlisting}

\section{Pretrained Models}
The Keras API offers easy access to pre-trained models through the applications library. It then allows removal of the top output layer. The pre-trained model without an output layer as seen in  \ref{pre-trainedmodel} can then be passed as a layer to a model with its output layer that will leverage from the pre-trained model weights. 

\begin{lstlisting}[language=Python, caption=Implemeting a pre-trained model called inception with the Keras API, label=pre-trainedmodel]
base_model = tf.keras.applications.inception_v3.InceptionV3(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

\end{lstlisting}

\section{Custom Layers}

%todo more on the difference between LRN and batch normalization%
Some custom layers were needed like the local response normalization layer as described in \citep{NIPS2012_4824}.
Local response normalization is a non-trainable layer that normalizes values in a feature map within a local neighbourhood making less adaptive.
While it's more commoly used predecessor, batch normalization as discussed in \citep{ioffe2015batch} is a trainable normalization layer, which allows for a much higher learning rate and requires less care with initialization.
Batch normalization is also said in some cases to eliminate the need for Dropout.
Another benefit of the Keras API is how it allows for easy creation of custom layers through inheritance from its Keras layer class as seen in \ref{lrn}.
Here since all the parameters of the Local response normalization layer are fixed, no training is required. Hence, the trainable is set to false in the build function. The parameters for this layer are also set in the init method.
In the call method that defines the implementation of the layer, it utilizes the TensorFlow nn library API, which holds functions that can be applied to tensor.
Fortunately, the TensorFlow nn library also offers a local response normalization function calculation which returns the normalized tensor. This was then wrapped in the Keras custom layer,  so it can be used as a layer in the Keras sequential model to implement the models.

\begin{lstlisting}[language=Python, caption=Local response normalization layer implemetation, label=lrn]
    class MyLRNLayer(tf.keras.layers.Layer):
    def __init__(self, depth_radius=5,bias=1,alpha=1,beta=0.5, **kwargs):
    # self.output_dim = output_dim
    self.depth_radius = depth_radius
    self.bias = bias
    self.alpha = alpha
    self.beta = beta
    super(MyLRNLayer, self).__init__(**kwargs)

    def build(self, input_shape):
    # Create a trainable weight variable for this layer.
    self.kernel = self.add_weight(name='kernel',
    shape=(None),
    initializer='uniform',
    trainable=False)
    super(MyLRNLayer, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
    return tf.nn.local_response_normalization(x,self.depth_radius,self.bias,self.alpha,self.beta)
\end{lstlisting}

In addition to this layer some custom python helper methods were also used for the formatting of the images. Some of this formating includes reducing the image size from $256 \times 256 \times 3$ to $170 \times170 \times 3$ as suggested in \citep{KarpathyCVPR14}.
These helper methods were also used for selecting the frames required for each model and for normalizing the image pixels.


\section{Optimizations}

As mentioned previous, the models in \citep{KarpathyCVPR14} use the local response normalization \cite{ROBINSON20071631} also used in the alexnet paper\cite{NIPS2012_4824}. However, the  local response normalization is now considered obsolete and the batch normalization \cite{ioffe2015batch} is the new standards normalization layer with some optimization benefits. Hence the models were also reconstructed using the batch normalization which is a layer also offered by Keras API,

Drawing from \citep{KarpathyCVPR14} downpour stochastic gradient descents setup. The stochastic gradient descents optimization was set up using the Keras optimizers method offered by the Keras API. This allowed for the easy setup of the optimizer using the momentum of 0.9, a decay of 0.0005 and a learning rate of $1e^{-3}$, as seen in \ref{sdgKeras}

\begin{lstlisting}[language=Python, caption=Keras SGD optimization, label=sdgKeras]
    sgd = tf.keras.optimizers.SGD(lr=1e-3, momentum=0.9, decay=0.0005)
\end{lstlisting}

% what is this, it is gradient descet to do with multiple distributed systems (http://ruder.io/optimizing-gradient-descent/index.html#tensorflow%

Keras also offers a range of optimizers such as Adam optimization algorithm as described in \citep{kingma2014adam} \citep{ruder2016overview} and the RMS prop optimization algorithm also described in \citep{ruder2016overview}.
 %These two optimization where tested against some models to explore changes in performance.%

%todo explain more  sparse_categorical_crossentropy and get reference for categorical crossentropy and sparse_categorical_crossentropy %
The Keras API also provides a range of losses such as sparse categorical cross-entropy, which was used for most models.  Keras also provides a utility method that converts label to a categorical format. However, this is only needed when using categorical cross-entropy loss. For sparse categorical cross-entropy loss, the categories could be left as integers  The option of loss was passed as a parameter in the module compile function as seen in \ref{loss}. Keras has a host of different performance metrics and also allows for the use of custom metrics; however, a standard accuracy metric was used. 

\begin{lstlisting}[language=Python, caption=Compiling model with sparse categorical crossentropy loss , label=loss]
    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])
\end{lstlisting}


The Keras API also provides other functions to help with performance such as dropout and data augmentation to help with overfitting.

\subsection{Dropout}
The Keras API offers dropout in the form of a Keras layer that can be added after the layer requiring dropout. This was used in the case of the pre-trained models as seen in \ref{dropout}, where a dropout of 50\% was added.

\begin{lstlisting}[language=Python, caption=Application of dropout to pretrained models, label=dropout]
    model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.Dropout(0.5),
    global_average_layer,
    prediction_layer
    ])
\end{lstlisting}

\subsection{Early stopping}

Keras also allows for early stopping by implementing a callback function which is passed to the model a few of these callbacks were implemented as seen in \ref{callback} for different experiments. Here in \ref{callback}, the model is set to stop training once an accuracy of 99\% is reached on the training data. In a way, early stopping can also be implemented by selecting the number of epochs for which the model should train. 

\begin{lstlisting}[language=Python, caption=Implementing early stopping at 99\% accurracy with keras call back, label=callback]
    class highAccCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc')>0.999):
    print("\nReached 99.9% accuracy so cancelling training!")
    self.model.stop_training = True

\end{lstlisting}

%todo talk about data augementation%
% Other hyperparamenrts used to improve performance included the  the learning rate, dropout, initial values and batch size.

% \section{Pretained model}
%talk about how they are implement and the softmax used



\subsection{Image Augmentation}
Some image augmentation was implemented as described in \citep{KarpathyCVPR14}. \citep{KarpathyCVPR14} talks about using the following data augmentation to reduce overfitting which includes applying  images cropping at center region, resizing the image to a $200 \times 200$ size then randomly sampling a $170 \times 170$ region, and finally randomly flipping the images horizontally with $50\%$ probability.
As the last step of preprocessing, \citep{KarpathyCVPR14} also discussed subtracting a constant value of 117 from raw pixel values, which was the approximate value of the mean of all pixels in the images used.
%todo reference for thsis
As much of this was recreated, with the exception of subtracting the approximate value of the mean of all pixels in the images in UCF101 dataset.  The pixels in the images were instead divided by 225.  As this is a popular technique of normalizing images.
The Keras API offers an image augmentation method. However this was not as flexible as needed. Instead, the TensorFlow image library functions as seen in \ref{kerasAug} were used to apply the require augmentation. 

\begin{lstlisting}[language=Python, caption=keras image augumentation, label=kerasAug]
    def centerCropAndRezise(dataset):
    image = dataset["video"]
    centerImage = tf.image.central_crop(image,0.5)
    resizedImage = tf.image.resize(centerImage, [200,200])
    randomImage =  tf.image.random_crop(resizedImage,(170,170,3),seed=None,name=None)
    flipImage = tf.image.random_flip_left_right(randomImage, seed =None)
    dataset["video"] = flipImage
    return dataset
\end{lstlisting}




\section{Computing Power}
%  \subsection{Computation Power}
%different learning rates for adam including the same usd for the s
Once familiar with the Keras API the models were then moved to python scripts for running on the UCL clusters. 
Batch jobs were set up to run on the python script on the clusters. 
The RAM size and number of GPUs used for running the models were also setups using the batch job script.  The memory used for most jobs was about 128GB, and 2 GPUs was typically used.  The clock time of most jobs ranged from 20 hours to 12 hours. This was typically estimated from the time take to run one epoch, and the number of epochs required in the model script. The software also needed to run the models on a node such as the TensorFlow GPU distribution was also loaded using the batch script. 

\begin{lstlisting}[ caption=UCL cluster batch job script example, label=batchscript]
#!/bin/bash -l
# Batch script to run a GPU job on Legion under SGE.
# 0. Force bash as the executing shell.
#$ -S /bin/bash
# 1. Request a number of GPU cards, in this case 2 (the maximum)
#$ -l gpu=2
# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=12:00:0
# 3. Request 1 gigabyte of RAM (must be an integer)
#$ -l mem=128G
# 4. Request 15 gigabyte of TMPDIR space (default is 10 GB)
#$ -l tmpfs=15G
# 5. Set the name of the job.
#$ -N batch_single_frame_Job
# 6. Set the working directory to somewhere in your scratch space. This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID :)
#$ -wd /home/zceef06/Scratch/output
# 7. Your work *must* be done in $TMPDIR
cd $TMPDIR
# 8. load the cuda module (in case you are running a CUDA program
module unload compilers mpi
module load compilers/gnu/4.9.2
module load python3/recommended
module load cuda/10.0.130/gnu-4.9.2
module load cudnn/7.4.2.24/cuda-10.0
module load tensorflow/1.14.0/gpu
# 9. Run the application - the line below is just a random example.
python /home/zceef06/msc-project/models/singleFrameBatch.py
# 10. Preferably, tar-up (archive) all output files onto the shared scratch area
tar zcvf $HOME/Scratch/files_from_job_$JOB_ID.tar.gz $TMPDIR
# Make sure you have given enough time for the copy to complete
\end{lstlisting}


%On the GPU simple models such as the single frame model ran between 40 - 30 minutes per epoch while more complex models such as the slow and late fusion model ran on average about 1 hour 45 minutes - 3 hours per epochs.%